---
title: "Data Analysis"
output: html_notebook
---

With a cleaned (tidy) dataset ready to go and with some detailed exploratory data analysis out of the way, we're ready to test hypotheses. Rather than focus on the details of a specific programming language, this session will help you build your statistical modeling skills. We'll talk about how to build models for different types of data and show how each model can be implemented in R or Python.

To get started, let's load the clean data. You can do this with R or Python. I'm going to load it in R and call it up in Python when I need to (there's no need to load it with both languages).

```{r include = FALSE}
library(tidyverse)
flight_data <- read_csv("/Users/nickjenkins/Documents/Workshops & Conferences/Data-Science-Workshop/Winter 2022/data/flights_cleaned.csv")
```

```{r eval = FALSE}
library(tidyverse)
flight_data <- read_csv("Documents/my_project/flights_cleaned.csv")
```

# The General Linear Model

During our data exploration we investigated several interesting questions, but when we move on to modeling our data we want to investigate the relationship between variables in a more precise fashion. That's where statistics comes in. Statistics allows us to talk about relationships between multiple variables in terms of probability which helps to quantify our uncertainty of any specific answer we might provide.

In the flight data, we investigated patterns in the data with visual methods but now we want to build statistical models to test these patterns. Let's start simple. Are planes that depart late more likely to arrive late? let's reproduce the plot we made between these variables last time, using `Matplotlib` this time instead of `ggplot2`:

```{python}
import matplotlib.pyplot as plt
plt.scatter(r.flight_data["dep_delay"], r.flight_data["arr_delay"])
plt.xlabel("Departure Delays")
plt.ylabel("Arrival Delays")
plt.show()
plt.clf()
```

To quantify this relationship, we can use a regression model. Let's start with linear regression. Linear regression is the first model most students learn and they learn to write it like this:

$$
y_i = \alpha + \beta x_i + \epsilon_i
$$

where $y_i$ is the outcome variable, $\alpha$ is the estimated intercept, $\beta$ is the estimated slope, $x_i$ is a column of data, and $\epsilon_i$ is the error term which measures the difference between the observed and predicted data for each row of data.

It's not wrong to write your model like this, but it hides a lot of assumptions that you are making about your model. For example, this model assumes that your data is normally distributed. One way of writing this is like this:

$$
\begin{aligned}
y_i &= \alpha + \beta x_i + \epsilon \\ 
\epsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$

This is better because we can see that we are assuming that the errors are normally distributed with a mean of 0 and a constant variance (homoscedastic). However, writing our models this way really only works when we're working with normally distributed data. The even better, more general way to write this model is like this:

$$
\begin{aligned}
y_i &\sim \text{Normal}(\mu_i, \sigma^2) \\
\mu_i &= \alpha + \beta x_i
\end{aligned}
$$

This tells us that our model assumes that the data are normally distributed, has a constant variance, and we see the predictors that we are using to predict the mean. Cool, but how to do implement this in software? Let's see how starting with R:

```{r}
fit_r <- glm(formula = arr_delay ~ dep_delay,
             family = gaussian(link = "identity"),
             data = flight_data)
```

Let's break this down. We're using the `glm` function, which stands for generalized linear model, and a Gaussian family, which is the normal distribution, and an "identity" link function, which we'll talk more about later. Finally, we just feed in the data with `data =`.

Now let's see this in Python:

```{python}
import statsmodels.formula.api as smf 
import statsmodels.api as sm

fit_py = smf.glm(formula = "arr_delay ~ dep_delay", 
                 family = sm.families.Gaussian(sm.families.links.identity()),
                 data = r.flight_data)

fit_py_fit = fit_py.fit()
```

We can view the results in R like this:

```{r}
summary(fit_r)
```

and in Python like this:

```{python}
print(fit_py_fit.summary())
```

The normal distribution is a good choice when we're modeling data that is continuous and can theoretically range from $-\infty$ to $\infty$. That fits a lot of scenarios but not all scenarios.[^1] One of the most common cases where the normal distribution isn't great is if we are trying to model a binary outcome.

[^1]: Check this article on why the normal distribution is so common: (<https://towardsdatascience.com/why-is-the-normal-distribution-so-normal-e644b0a50587>)\[<https://towardsdatascience.com/why-is-the-normal-distribution-so-normal-e644b0a50587>\]

# Generalizing the Linear Model

In order to generalize our linear model to model any type of data, we need two components: a likelihood function and a link function.

## What is a likelihood function?

Likelihood functions are essentially probability models of our data. They describe the data generating process. Technically, they show what the parameter values need to be in our model to maximize the chances of observing the data that we have. That's technical, but just remember that one of the main goals of statistics is to build models that can be make accurate predictions and describe data. We do this by choosing likelihood functions that describe the observed data.

## What is a link function?

The link function restricts the range of the linear equation to the range of the outcome variable. This makes it so that our model will produce predictions that make sense. It wouldn't be good if we were trying to model the probability of someone winning an election and the model predicted a negative value! Or, if it gave a value greater than 1! Link functions prevent this from happening.

## Putting it All Together

Our models almost always use the same linear structure ($\mu_i = \alpha + \beta x_i$), but we can choose any likelihood and link functions that we need to best match the data we are modeling. With these pieces we can **generalize** our linear equation to model any type of data we want, hence the **generalized linear model**.

# Binomial Regression

For example, let's suppose that we wanted to build a model to understand why delays happen. Maybe planes that preparing to travel shorter distances are more likely to get delayed since shorter travel times mean faster turn around times. Let's generate a binary indicator for whether a flight was delayed that will serve as the outcome variable:

```{r}
flight_data <- flight_data %>% 
  mutate(delay_dum = ifelse(dep_delay > 0, yes = 1, no = 0))
```

Which probability distribution would best match this data generating process? Here we have a 0-1 outcome so our data generating process is limited to either 0 or 1. This means that we need a discrete distribution that produces values that are either 0 or 1. So, we have two questions to answer:

1.  What likelihood function (probability distribution) should we use?
2.  What link function should we use?

The binomial distribution is a good choice for a likelihood in this case because it is a distribution of discrete values of either 0 or 1. So far, this means that our equation looks like this:

$$
\begin{aligned}
y_i &\sim \text{Binomial}(1, p_i) \\
p_i &= \alpha + \beta x_i
\end{aligned}
$$

Let's plot a binomial distribution to get an idea of what it looks like. Suppose we flipped a coin 3 times. Here is a visual (using the binomial distribution) of the probability of getting 0, 1, 2, and 3 heads:

```{r}
library(tidyverse)

binomial_data <- tibble(
  # number of trials
  n = 3,
  
  # probability of success on a given trial
  p = 0.5,
  
  # range of successes (heads)
  s = 0:3,
  
  # probability density
  prob = dbinom(x = s, size = n, prob = p)
)

ggplot(data = binomial_data, aes(x = s, y = prob)) +
  geom_col() +
  labs(x = "# of Successes", y = "Probability",
       title = "The Binomial Distribution")
```

Now, what if we try this 100 times?

```{r}
binomial_data <- tibble(
  # number of trials
  n = 100,
  
  # probability of success on a given trial
  p = 0.5,
  
  # range of successes (heads)
  s = 0:100,
  
  # probability density
  prob = dbinom(x = s, size = n, prob = p)
)

ggplot(data = binomial_data, aes(x = s, y = prob)) +
  geom_col() +
  labs(x = "# of Successes", y = "Probability",
       title = "The Binomial Distribution")
```

Ok awesome, the binomial distribution seems like a good choice. Here's the problem though. The linear equation ($p_i = \alpha + \beta x_i$) can give us just about any value we want even though we only want values between 0 and 1. In other words, we're using a probability distribution that produces 0's and 1's but there's nothing stopping our model's predictions for $p_i$ from staying between 0 and 1. That's where the *link function* comes in.

Remember how we entered `family = gaussian(link = identity)` for the linear regression? When you estimate a linear regression, you're technically using the *identity link function*, which basically means that you aren't using a link function. When our data is discrete, one type of link function we could use is the *logit link function*. The logit link function restricts $p_i$ to fall between 0 and 1. Mathematically, the logit link function is defined as:

$$
\ln \big(\frac{p_i}{1 - p_i}\big)
$$

This simple equation ensures that we always get a prediction between 0 and 1.

Let's see the link function in action:

```{r}
ggplot(data = tibble(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = function(x) log(x / (1 - x)), 
                geom = "line") +
  coord_flip() +
  labs(y = "x", x = "logistic(x)", title = "Plotting the Logit Link Function")
```

Alright, let's fit the model already!

In R:

```{r}
binom_fit_r <- glm(formula = delay_dum ~ distance,
                   family = binomial(link = "logit"),
                   data = flight_data)
summary(binom_fit_r)
```

In Python:

```{python}
binom_py = smf.glm(formula = "delay_dum ~ distance", 
                   family = sm.families.Binomial(sm.families.links.logit()),
                   data = r.flight_data)

binom_fit_py = binom_py.fit()
print(binom_fit_py.summary())
```

Great! Now how do we interpret the estimates? The easiest way to interpret them is to convert them to the probability scale (instead of talking about log-odds, or odds ratios). We do this by "undoing" the link function. The inverse logit link function is the *logistic* function and we can call it up in R with `plogis()` or in Python by defining the inverse logit function manually.

So, a one unit increase in distance is associated with a:

```{r}
plogis(9.865e-05)
```

increase in the relative probability of being delayed. Or with Python:

```{python}
import numpy as np

def inv_logit(p):
    return np.exp(p) / (1 + np.exp(p))

inv_logit(9.865e-05)
```

Considering that a 50% probability is basically a coin flip, `distance` doesn't really have any predictive power.

## Exercise

1.  What is the likelihood function and the link function for this model:

    $$
    \begin{aligned}
    y_i &\sim \text{Binomial}(1, p_i) \\
    \text{probit}(p_i) &= \alpha + \beta x_i
    \end{aligned}
    $$

2.  Use Python or R to fit the model above and print the results.

# Poisson Regression

What if our research question asked how if the number of delays depended on the month of the year? We would need a different model for this. Let's prepare the data first. To do that, we'll use our tidyverse skills to calculate the number of delays each month.

```{r}
flight_data_agg <- flight_data %>% 
  group_by(year, month) %>% 
  summarize(num_delays = sum(delay_dum))

head(flight_data_agg)
```

Now we have a data generating process that produces discrete positive values from 0 to $\infty$.

1.  What distribution (likelihood function) describes a process like this?
2.  What link function should we use?

For this situation, the Poisson distribution is a good choice for a likelihood since it describes a discrete process that generates a count of events taking place. This means that the Poisson distribution is defined for values greater than all the way to $\infty$. In other words, it works for all positive values. Technically, the Poisson distribution is undefined for values of 0. So if we had a data generating process that produced positive values **including** 0, we would need to use a zero-inflated Poisson likelihood function.

Let's see what the Poisson distribution looks like:

```{r}
poisson_data <- tibble(
  # range of events
  s = 0:20,
  
  # lambda (mean number of event occurances)
  rate = 2,
  
  # probability density
  prob = dpois(x = s, lambda = rate)
)

ggplot(data = poisson_data, aes(x = s, y = prob)) +
  geom_col() +
  labs(x = "# of Events", y = "Probability",
       title = "The Poisson Distribution")
```

But how do we map the linear predictor to the range of all possible values? We'll use the *log link function* since the log function is only defined for positive values greater than 0.

Let's visualize this link function:

```{r}
ggplot(data = tibble(x = c(0, 8)), aes(x = x)) +
  stat_function(fun = function(x) log(x), 
                geom = "line") +
  coord_flip() +
  labs(y = "x", x = "exp(x)", title = "Plotting the Log Link Function")
```

It's always positive!

Mathematically, the Poisson model looks like this:

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \alpha + \beta x_i
\end{aligned}
$$

Here is the fitted model in R:

```{r}
poisson_fit_r <- glm(formula = num_delays ~ month,
                     family = poisson(link = "log"),
                     data = flight_data_agg)
summary(poisson_fit_r)
```

And in Python:

```{python}
poisson_py = smf.glm(formula = "num_delays ~ month",
                     family = sm.families.Poisson(sm.families.links.log()),
                     data = r.flight_data_agg)

poisson_fit_py = poisson_py.fit()

print(poisson_fit_py.summary())
```

This isn't the best model because it doesn't really make sense to treat `month` as a continuous variable, but we're not worried about that here.

Now, how do I interpret THIS model? Just like before, we need to "undo" the link function. In this case, the exponential $\exp()$ is the opposite of the *log link function*.

So, a one-unit increase in month (which doesn't really make sense) is associated with a

```{r}
(1 - exp(0.0020939)) * 100
```

percent decreased in the number of delayed flights.

In Python it's:

```{python}
import numpy as np
(1 - np.exp(0.0020939)) * 100
```

# Modeling with Other Distributions

We talked about Poisson and Binomial, but there are plenty of other likelihood functions you can use. For example, need to model a proportion? Something like, say, the percentage of a city that gets stopped by the police? For that you would want to use the beta distribution.

The beta distribution is for continuous data that can only be between 0 and 1. What link function would we use? Well, you already know of a link function that will limit the linear predictor to be between 0 and 1 - the logit link function. You can fit a beta regression in R with the [`betareg`](https://rdrr.io/cran/betareg/man/betareg.html) package.

What if your outcome consists of different unordered categories? For that you'd need the categorical distribution which you can do with the [`nnet`](https://www.rdocumentation.org/packages/nnet/versions/7.3-17/topics/multinom) R package.

# Resources for Further Study

To learn more about generalized linear models, I'd recommend these resources:

-   [Beyond Multiple Linear Regression](https://bookdown.org/roback/bookdown-BeyondMLR/)

-   [Regression and Other Stories](https://assets.cambridge.org/97811070/23987/frontmatter/9781107023987_frontmatter.pdf)
